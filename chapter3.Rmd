# Logistic regression
```{r, include=FALSE}
# Load all the libraries, but do not show them in the rendered version
library(GGally)
library(ggplot2)
library(dplyr)
library(tidyr)
library(gridExtra)
library(boot)

# Prescribe the seed for reproducible results
set.seed(177)
```

*A short summary and things learned*

- In the exercise we use the information about students of two Portuguese schools to build a statistical model for predicting high alcohol consumption rate
- For modeling a binary target variable we use logistic regression
- Model quality is assessed by investigating prediction and training errors
- In attempt to find a better set of explanatory variables we use a set of randomly selected models to minimize the prediction error

Again, before we start the analysis we load the data set from the file system.
This data set provides information about students of two Portuguese schools and their achievements.
It contains received grades (variables `G1`, `G2` and `G3`) as well as students background including social and school-related information.
The original data set could be obtained via the following link <https://archive.ics.uci.edu/ml/datasets/Student+Performance> and detailed description of the data set variables is also provided.
Although the original data set was provided separately for two subjects: Mathematics and Portuguese language, in the current study a preprocessed data set is used.
Ths data set is obtained by merging the original data sets.

Variables in the perprocessed data set could be shown vy applying the `names` function to the loaded data frame:
```{r}
alc <- read.csv('alc.csv')
dim(alc)
names(alc)
```
As could be seen from the output this data set introduces two additional variables, namely `alc_use` which is a simple mean of `Dalc` and `Walc`, and `high_use` which indicates relatively high alcohol consumption by a student and defined to be `r TRUE` when `alc_use` takes values greater than the threshold value of `2`.

Since the data set provides quite extensive information about students it could be possible to build a statistical model predicting high alcohol consumption based on these data.
For this study I choose the four following variables:
```{r}
selected_columns <- c('famrel', 'studytime', 'absences', 'goout')
```

where `famrel` describes quality of student's family relationships (with higher values corresponding to better relationships), `studitime` describes how much time students spends on their studies over a week, `absences` descries the number of times when a student was absent from school, and `goout` describes how often a students goes out with friends.

I expect these variables to be connected to the alcohol consumption since it would be natural to assume that a person with bad family relationships who does not spend too much time studying, often absent and going out with friends would show higher alcohol consumption.

To get a better overview of the selected variables and study their possible connection to the alchol consumption a graphical summary could be of a great help.

```{r, fig.align='center', fig.width = 6, fig.asp = 1.0}
alc %>% 
pivot_longer(all_of(selected_columns), names_to='key', values_to='value') %>% 
ggplot(aes(value, fill=high_use)) + 
  facet_wrap("key", scales = "free") + 
  geom_bar() + 
  theme_bw() +  
  theme(panel.grid.major=element_blank(), panel.grid.minor=element_blank())
```

The bar plots of the selected variables show no striking features supporting the initial hypothesis.
Although the `goout` tends to have higher values for students who have high alcohol consumption.
Surprisingly the `absences` does not seem to show peaks of high absence for students with high alcohol consumption but rather features a gradual increase of drinkers fraction with increasing days of absence.
For the `studytime` variable the fraction of students with high alcohol consumption greatly diminishes with increasing number of hours spent on studies.
And finally `famrel` shows a somewhat similar picture with relatively less number of drinking students in families with good relationships.

This information could also be provided in tabular form which supports the conclusions drawn from the graphs:
```{r}
for(var in selected_columns) {
  print(table(alc[c('high_use', var)]))
}
```


To predict the high alcohol consumption based on the selected variables a logistical model could applied:
```{r}
f_string <- paste('high_use ~ ', paste(selected_columns, collapse = ' + '))
model <- glm(as.formula(f_string), data = alc, family = "binomial")
summary(model)
```

summary of the trained model indicate that all the selected predictors have statistically significant relation with the target variable `high_use` taking the significance level $\alpha = 0.05$.
Coefficients of the model suggest that `absences` has the weakest connection with the target variable whereas the other selected explanatory variables show a more strong link.

A bit more insight on the characteristics of the obtained model could be learned by studying the coefficients in form of odd ratios and providing their confidence intervals:

```{r}
odds_ratio <- model %>% coef    %>% exp
confidence <- model %>% confint %>% exp
cbind(odds_ratio, confidence)
```

this table shows that for `famrel` and `studytime` odds ratios are below one (namely, `r format(odds_ratio['famrel'], digits=3)` and `r format(odds_ratio['studytime'], digits=3)`) which means that increasing these parameters would decrease the probability of student being a drinker.
Contrary, the odds ratios for `absences` and `goout` are above one thus increasing these parameters would increase the probability of high alcohol consumption by a student.
Another important observation from this table is that the confidence intervals for all the odds ratios do not include 1 meaning that these odds ratios are significant.
This is true also for the `absences` variable which has the odds ratio of `r format(odds_ratio['absences'], digits=3)`, which is rather close to 1 but still significant.

To investigate the predictive power of our model it could be used to estimate the probability of high alcohol consumption from the same data set which was applied to train the model.
To do so, the `predict` function can be used and if the resulting probability if higher than 0.5 a student is assumed to be a drinker.
This information combined with the actual data on alcohol consumption allows generating the cross tabulation of predicted vs actual values:
```{r}
probabilities <- predict(model, type = "response")
alc <- alc %>% mutate(probability = probabilities, prediction = probability > 0.5)

table(high_use = alc$high_use, prediction = alc$prediction)
```

This table shows an interesting property of the model, it tends to predict the low alcohol consumption better than high one i.e. it gives a considerably  high number of false negatives.
This property of the model is apparent from the following plot:

```{r}
alc %>%
ggplot(aes(x = probability, y = high_use)) +
  geom_violin(trim = TRUE, fill='#DCDCDC') +
  geom_vline(xintercept = 0.5) +
  xlim(0,1) +
  theme_bw() +
  theme(panel.grid.major=element_blank(), panel.grid.minor=element_blank())
```

it is evident that for students having low alcohol consumption the model tends to predict consistent low probability of being a drinker.
From the other side for drinking students the model yields a somewhat close to uniform distribution of predictions showing no predictive skill.

The total proportion of the failed predictions (i.e. the training error) could be calculated in the following way by taking a simple average of model predictions:
```{r}
loss_func <- function(class, prob) {
  n_wrong <- abs(class - prob) > 0.5
  mean(n_wrong)
}

loss_func(class = alc$high_use, prob = alc$probability)
```
```{r, include=FALSE}
model_training_error <- loss_func(class = alc$high_use, prob = alc$probability)
```

which gives a value of `r format(model_training_error, digits=3)` meaning that about a quarter of the predictions were incorrect.

Such an approach which uses the whole data set to train a model and the to test it, though simple and straightforward, gives a biased estimates of model quality because all the data points which are used for testing are already 'known' to the model since they were used for training.
A more robust approach takes only a part of the data set for training the model and keeps a part of it for testing.
This approach is applied in the `cv.glm` function which estimates the model prediction error in presence of unknown observations.
There the data set is divided into 10 chunks and only 9 of them are used for model training when the last one is used for validation.
This procedure is repeated for all combinations of training and validation chunks returning a K-fold cross-validation prediction error.
```{r}
cv <- cv.glm(data = alc, cost = loss_func, glmfit = model, K = 10)
cv$delta[1]
```

For the model built in the current exercise the cross-validation procedure gives the prediction error of `r format(cv$delta[1], digits=3)` which is higher than the previous estimate because it use unknown to the model data for validation.
This error is lower than the error of the example model (it use the following formula: `high_use ~ sex + failures + absences`) given for this course which was about 0.26.

But giving a relatively big number of potential explanatory variables it is not a straightforward task to select the best set for building a model.
A naÃ¯ve solution would be testing all the possible combinations of the explanatory variables and taking a model which gives the lowest prediction error.
The problem is that for a data set with `r dim(alc)[2] - 3` free parameters the number of possible combinations of explanatory variables would be:

$$
C = \sum_r^{`r dim(alc)[2] - 3`} \frac{`r dim(alc)[2] - 3`!}{r!\cdot(`r dim(alc)[2] - 3` - r)!} \equiv `r sum(factorial(34)/(factorial(1:34)*factorial(34 - 1:34)))`
$$
which is rather impractical.
But to assess the properties of models build with different explanatory variables only a minor fraction of all possible combinations would suffice.
Therefore to investigate the models built with different combinations of different number of explanatory variables the following approach is applied.
For each size of the explanatory vector a random selection of the explanatory variables is used to train a model and obtain the model training and prediction errors.
This procedure is repeated `N` times for each explanatory vector size to get multiple model realizations.
For each model the model formula, size of the explanatory vector, prediction and training errors are stored in the data frame for the following analysis.

```{r, cache=TRUE}
possible_predictors <- head(names(alc), n = -3)
possible_predictors <- possible_predictors[! possible_predictors %in% c('Walc', 'Dalc', 'alc_use')]

mdf <- data.frame(numeric(0), logical(0), numeric(0), character(0), stringsAsFactors = FALSE)
names(mdf) <- c('size', 'training', 'error', 'model')

num_possible_predictors <- length(possible_predictors)
for(i in 30:1) {
  # Do not test all the combinations for models with more than
  # three predictors, it takes way too much time.
  if(i <= 3) {
    selected_predictors <- gtools::combinations(
        n=num_possible_predictors
      , r=i
      , v=possible_predictors
      , repeats.allowed=FALSE)
  } else {
    selected_predictors <- t(replicate(400, sample(possible_predictors, size = i)))
  }

  for(r in 1:nrow(selected_predictors)) {
    predictors <- selected_predictors[r,]
    
    f_string <- paste('high_use ~ ', paste(predictors, collapse = '+'))
    m <- tryCatch(
      glm(as.formula(f_string), data = alc, family = "binomial"), 
      error=function(e){return(NULL)}, 
      warning=function(w) {return(NULL)})
    
    if(!is.null(m)) {
      # Calculate training error
      alc_loc <- data.frame(alc)
      p <- predict(m, type = "response")
      alc_loc <- alc_loc %>% mutate(probability = p, prediction = probability > 0.5)
      loss <- loss_func(class = alc_loc$high_use, prob = alc_loc$probability)
      
      cv <- cv.glm(data = alc_loc, cost = loss_func, glmfit = m, K = 10)

      # Store training and testing errors
      mdf[nrow(mdf) + 1,] <- list(i, TRUE,  loss,        f_string)
      mdf[nrow(mdf) + 1,] <- list(i, FALSE, cv$delta[1], f_string)
    }
  }
}
```

As could be seen from the summary figure, the model errors tend become smaller with increasing the size of the explanatory vector (there `training` indicates training errors if `r TRUE` and prediction errors if `r FALSE`).
This could be attributed to the non-totality of the selected approach which makes it easier to draw a set of 'bad' variables if the explanatory vector is short.
From the other size for long explanatory vectors there is a higher probability of getting 'good' variables from a random draw.

```{r, fig.align='center', fig.width = 8, fig.asp = 0.5}
mdf$size <- as.factor(mdf$size)
mdf %>% 
ggplot(aes(x = size, y = error, col=training)) + 
  geom_boxplot() + 
  theme_bw() +
  theme(panel.grid.major=element_blank(), panel.grid.minor=element_blank())
```

From all the tested models, the following give the lowest prediction error:
```{r}
mdf[mdf$training == FALSE,] %>% slice_min(error)
```

The model with the the lowest prediction error (there the model with fewer explanatory variables was taken) shows some improvement in the prediction skill though still tend to give a considerable amount of false negative predictions according to the provided summary:
```{r}
best_score <- mdf[mdf$training == FALSE,] %>% slice_min(error)
best_formula <- as.formula(best_score$model[2])
better_model <- glm(as.formula(best_formula), data = alc, family = "binomial")
summary(better_model)

p2 <- predict(better_model, type = "response")
a2 <- data.frame(alc)
a2 <- a2 %>% mutate(probability = p2, prediction = probability > 0.5)

table(high_use = a2$high_use, prediction = a2$prediction)

a2 %>%
ggplot(aes(x = probability, y = high_use)) +
  geom_violin(trim = TRUE, fill='#DCDCDC') +
  geom_vline(xintercept = 0.5) +
  xlim(0,1) +
  theme_bw() +
  theme(panel.grid.major=element_blank(), panel.grid.minor=element_blank())
```

